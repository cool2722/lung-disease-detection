{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Sex: \n",
      "Patient Age: Unknown\n",
      "Patient Size (m): Unknown\n",
      "Patient Weight (kg): Unknown\n"
     ]
    }
   ],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "\n",
    "def extract_patient_details(dicom_path):\n",
    "    \"\"\"Extracts patient sex, age, size, and weight from a dataset with DICOM files.\"\"\"\n",
    "\n",
    "    image_to_detail_list = np[[file:{}]]\n",
    "    for file in dicom_path:\n",
    "        dicom_data = pydicom.dcmread(file)\n",
    "\n",
    "        patient_details = {\n",
    "            \"Patient Sex\": dicom_data.PatientSex if 'PatientSex' in dicom_data else \"Unknown\",\n",
    "            \"Patient Age\": dicom_data.PatientAge if 'PatientAge' in dicom_data else \"Unknown\",\n",
    "            \"Patient Size (m)\": dicom_data.PatientSize if 'PatientSize' in dicom_data else \"Unknown\",\n",
    "            \"Patient Weight (kg)\": dicom_data.PatientWeight if 'PatientWeight' in dicom_data else \"Unknown\",\n",
    "        }\n",
    "\n",
    "        np[[file:patient_details]]\n",
    "\n",
    "    return patient_details\n",
    "\n",
    "# Example usage\n",
    "dicom_file = \"../dataset\" \n",
    "patient_info = extract_patient_details(dicom_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Filename Sex   Age Size Weight\n",
      "0  07c12d0f562f17579aabc18c11e2ad54.dicom   M  051Y  NaN    NaN\n",
      "1  d1605d4007fbbdbec96acce4a834d10b.dicom   F  049Y  NaN    NaN\n",
      "2  0129a93b23aa71e7ea2b8b988d1c5287.dicom   O   NaN  NaN    NaN\n",
      "3  da922b5ee573e770260d4f6c849a17a5.dicom   F  049Y  NaN    NaN\n",
      "4  54d7d530808ec64ea19e9d4ab37cf579.dicom   M   NaN  0.0    0.0\n",
      "                                      Filename        Sex           Age         Size       Weight\n",
      "\u001b[0m07c12d0f562f17579aabc18c11e2ad54.dicom\u001b[0m \u001b[96mM\u001b[0m \u001b[95m051Y\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0md1605d4007fbbdbec96acce4a834d10b.dicom\u001b[0m \u001b[96mF\u001b[0m \u001b[95m049Y\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m0129a93b23aa71e7ea2b8b988d1c5287.dicom\u001b[0m \u001b[96mO\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0mda922b5ee573e770260d4f6c849a17a5.dicom\u001b[0m \u001b[96mF\u001b[0m \u001b[95m049Y\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m54d7d530808ec64ea19e9d4ab37cf579.dicom\u001b[0m \u001b[96mM\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92m0.0\u001b[0m \u001b[93m0.0\u001b[0m\n",
      "\u001b[0m8843bf18d7fc51c9389e414c28237333.dicom\u001b[0m  \u001b[96m\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0mcf39c28c29e15231c3956e0f83a8bf97.dicom\u001b[0m \u001b[96mF\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m938ce965d372654fbfcc844ecd3e77ca.dicom\u001b[0m \u001b[96mO\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m577a21d75105a48cf206ca39a454fe07.dicom\u001b[0m \u001b[96mM\u001b[0m \u001b[95m000Y\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m74292e695d6b5868b89acf26363ee93e.dicom\u001b[0m \u001b[96mM\u001b[0m  \u001b[95mnan\u001b[0m \u001b[92mnan\u001b[0m \u001b[93mnan\u001b[0m\n",
      "\u001b[0m07c12d0f562f17579aabc18c11e2ad54.dicom\u001b[0m\t\u001b[96mM\u001b[0m\t\u001b[95m051Y\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0md1605d4007fbbdbec96acce4a834d10b.dicom\u001b[0m\t\u001b[96mF\u001b[0m\t\u001b[95m049Y\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0m0129a93b23aa71e7ea2b8b988d1c5287.dicom\u001b[0m\t\u001b[96mO\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0mda922b5ee573e770260d4f6c849a17a5.dicom\u001b[0m\t\u001b[96mF\u001b[0m\t\u001b[95m049Y\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0m54d7d530808ec64ea19e9d4ab37cf579.dicom\u001b[0m\t\u001b[96mM\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92m0.0\u001b[0m\t\u001b[93m0.0\u001b[0m\n",
      "\u001b[0m8843bf18d7fc51c9389e414c28237333.dicom\u001b[0m\t\u001b[96m\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0mcf39c28c29e15231c3956e0f83a8bf97.dicom\u001b[0m\t\u001b[96mF\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0m938ce965d372654fbfcc844ecd3e77ca.dicom\u001b[0m\t\u001b[96mO\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0m577a21d75105a48cf206ca39a454fe07.dicom\u001b[0m\t\u001b[96mM\u001b[0m\t\u001b[95m000Y\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n",
      "\u001b[0m74292e695d6b5868b89acf26363ee93e.dicom\u001b[0m\t\u001b[96mM\u001b[0m\t\u001b[95mnan\u001b[0m\t\u001b[92mnan\u001b[0m\t\u001b[93mnan\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "max_files = 100\n",
    "numpy_filename = \"patient_details.npy\"\n",
    "columns = [\"Filename\", \"Sex\", \"Age\", \"Size\", \"Weight\"]\n",
    "\n",
    "df_patient_info = pd.DataFrame(np.load(\"../patient_details.npy\", allow_pickle=True) , columns= columns)\n",
    "print(df_patient_info.head())\n",
    "\n",
    "# ANSI colors per column\n",
    "colors = [\n",
    "    \"\\033[0m\",     # Filename – default\n",
    "    \"\\033[96m\",    # Sex – light cyan\n",
    "    \"\\033[95m\",    # Age – light magenta\n",
    "    \"\\033[92m\",    # Size – light green\n",
    "    \"\\033[93m\",    # Weight – light yellow\n",
    "]\n",
    "reset = \"\\033[0m\"\n",
    "\n",
    "# Create a copy for display with colored values\n",
    "df_colored = df_patient_info.copy()\n",
    "\n",
    "# Apply color to each column\n",
    "for i, col in enumerate(df_patient_info.columns):\n",
    "    df_colored[col] = df_patient_info[col].apply(lambda x: f\"{colors[i]}{x}{reset}\")\n",
    "\n",
    "# Print as a string (tabular format)\n",
    "print(df_colored.head(10).to_string(index=False))\n",
    "for i, (_, row) in enumerate(df_patient_info.iterrows()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    row = list(row)\n",
    "    colored_row = [f\"{colors[i]}{str(item)}{reset}\" for i, item in enumerate(row)]\n",
    "    print(\"\\t\".join(colored_row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### From Remote to Local (windows)\n",
    " scp root@194.164.196.246:/mnt/shared_dataset/  C:\\Users\\jsayed\\Downloads\\DHBW\\lung-disease-detection\n",
    " #### From Local to Remote (linux)\n",
    " scp ..\\..\\full_urls_max_rev.txt root@194.164.196.246:/mnt/shared_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files in source folder.\n",
      "Copied 500 files to '../dataset/train-500'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# === CONFIG ===\n",
    "num_samples = 500\n",
    "source_folder = \"../dataset/train\"         # Replace with your source folder\n",
    "destination_folder = f\"../dataset/train-{num_samples}\"   # Destination subfolder\n",
    "\n",
    "# Create destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get list of image files (you can filter by extension if needed)\n",
    "all_files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "print(f\"Found {len(all_files)} files in source folder.\")\n",
    "\n",
    "# Randomly sample files\n",
    "sampled_files = random.sample(all_files, min(num_samples, len(all_files)))\n",
    "\n",
    "# Copy files\n",
    "for filename in sampled_files:\n",
    "    src = os.path.join(source_folder, filename)\n",
    "    dst = os.path.join(destination_folder, filename)\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"Copied {len(sampled_files)} files to '{destination_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved final COCO JSON with:\n",
      " - 10755 images\n",
      " - 1398 annotations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Paths\n",
    "images_dir = f\"../dataset/train-{num_samples}\"\n",
    "# === Step 1: Load CSVs ===\n",
    "annotations = pd.read_csv(\"../dataset/annotations_train.csv\")\n",
    "image_labels = pd.read_csv(\"../dataset/image_labels_train.csv\")\n",
    "subset_filenames = {os.path.splitext(f)[0] for f in os.listdir(images_dir)}  # Folder with your images\n",
    "\n",
    "# === Build class mapping (28 classes: 0–27) ===\n",
    "disease_cols = image_labels.columns[2:]  # Skip image_id and rad_id\n",
    "class2idx = {name: i for i, name in enumerate(disease_cols)}\n",
    "idx2class = {i: name for name, i in class2idx.items()}\n",
    "\n",
    "# === Filter relevant annotations and assign class index ===\n",
    "annotations = annotations[annotations['image_id'].isin(subset_filenames)]\n",
    "annotations['category_id'] = annotations['class_name'].map(class2idx)\n",
    "\n",
    "# === Remove rows with NaN bbox ===\n",
    "annotations = annotations.dropna(subset=['x_min', 'y_min', 'x_max', 'y_max'])\n",
    "\n",
    "# === Compute vote weights ===\n",
    "vote_count = annotations.groupby(['image_id', 'class_name']).size().reset_index(name='votes')\n",
    "vote_count['weight'] = vote_count['votes'].apply(lambda v: 1 if v == 1 else 2 if v == 2 else 4)\n",
    "annotations = annotations.merge(vote_count[['image_id', 'class_name', 'weight']], on=['image_id', 'class_name'])\n",
    "\n",
    "# === Find true \"No finding\" images (2+ votes and no annotations) ===\n",
    "no_finding_votes = image_labels[image_labels[\"No finding\"] == 1]\n",
    "nf_votes = no_finding_votes.groupby(\"image_id\").size().reset_index(name=\"votes\")\n",
    "nf_ids = set(nf_votes[nf_votes[\"votes\"] >= 2][\"image_id\"])\n",
    "annotated_ids = set(annotations['image_id'])\n",
    "pure_no_finding = nf_ids - annotated_ids\n",
    "\n",
    "# === Build COCO JSON ===\n",
    "images = []\n",
    "annotations_out = []\n",
    "categories = []\n",
    "image_id_map = {}\n",
    "image_id_counter = 0\n",
    "annotation_id = 0\n",
    "\n",
    "# Categories\n",
    "for cid, cname in idx2class.items():\n",
    "    categories.append({\n",
    "        \"id\": cid,\n",
    "        \"name\": cname,\n",
    "        \"supercategory\": \"disease\"\n",
    "    })\n",
    "\n",
    "# Images with disease annotations\n",
    "for image_id in sorted(subset_filenames):\n",
    "    filename = image_id + \".png\"  # adjust if needed\n",
    "    image_id_map[image_id] = image_id_counter\n",
    "    images.append({\n",
    "        \"id\": image_id_counter,\n",
    "        \"file_name\": filename,\n",
    "        \"width\": 1024,\n",
    "        \"height\": 1024\n",
    "    })\n",
    "    image_id_counter += 1\n",
    "\n",
    "# Add annotations\n",
    "for _, row in annotations.iterrows():\n",
    "    image_id_num = image_id_map[row['image_id']]\n",
    "    x, y = float(row['x_min']), float(row['y_min'])\n",
    "    w = float(row['x_max'] - row['x_min'])\n",
    "    h = float(row['y_max'] - row['y_min'])\n",
    "\n",
    "    annotations_out.append({\n",
    "        \"id\": annotation_id,\n",
    "        \"image_id\": image_id_num,\n",
    "        \"category_id\": int(row['category_id']),\n",
    "        \"bbox\": [x, y, w, h],\n",
    "        \"area\": w * h,\n",
    "        \"iscrowd\": 0,\n",
    "        \"confidence\": int(row['weight'])\n",
    "    })\n",
    "    annotation_id += 1\n",
    "\n",
    "# No-finding images (add them without boxes)\n",
    "for nf_img in pure_no_finding:\n",
    "    if nf_img not in image_id_map:  # ensure it's not already added\n",
    "        filename = nf_img + \".png\"\n",
    "        images.append({\n",
    "            \"id\": image_id_counter,\n",
    "            \"file_name\": filename,\n",
    "            \"width\": 1024,\n",
    "            \"height\": 1024\n",
    "        })\n",
    "        image_id_counter += 1\n",
    "\n",
    "# Save JSON\n",
    "coco = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations_out,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "with open(\"annotations.json\", \"w\") as f:\n",
    "    json.dump(coco, f)\n",
    "\n",
    "print(\"✅ Saved final COCO JSON with:\")\n",
    "print(\" -\", len(images), \"images\")\n",
    "print(\" -\", len(annotations_out), \"annotations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
