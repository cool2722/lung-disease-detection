{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone YOLOv5 if not already present\n",
    "!if [ ! -d \"yolov5\" ]; then git clone https://github.com/ultralytics/yolov5.git; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install required packages\n",
    "!pip install -q -r yolov5/requirements.txt\n",
    "!pip install -q pydicom opencv-python pillow tqdm pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Step 3: Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from time import sleep  \n",
    "import subprocess\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Paths using Pathlib\n",
    "base = Path(\"/mnt/shared_dataset\")\n",
    "root = base / \"YOLO\"\n",
    "dicom_dir = base / \"physionet.org/files/vindr-cxr/1.0.0/train\"\n",
    "dicom_test_dir = base / \"physionet.org/files/vindr-cxr/1.0.0/test\"\n",
    "\n",
    "png_dir = root / \"images\"\n",
    "label_dir = root / \"labels\"\n",
    "test_dir = png_dir / \"test\"\n",
    "yaml_path = root / \"my-yolov5.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create required folders if not exist\n",
    "for sub in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(png_dir,sub), exist_ok=True)\n",
    "    os.makedirs(os.path.join(label_dir, sub), exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 3\n",
    "log_file = base / \"log_dir/ray_conversion_results.log\"\n",
    "\n",
    "# Setup root logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def convert_dicom_to_png_remote(dicom_path_str, png_path_str):\n",
    "    dicom_path = Path(dicom_path_str)\n",
    "    png_path = Path(png_path_str)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(dicom_path, force=True)\n",
    "            img = ds.pixel_array\n",
    "\n",
    "            if img.size == 0:\n",
    "                raise ValueError(\"Empty pixel data\")\n",
    "\n",
    "            img_normalized = ((img - img.min()) / (img.max() - img.min()) * 255).astype('uint8')\n",
    "\n",
    "            png_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            success = cv2.imwrite(str(png_path), img_normalized)\n",
    "\n",
    "            if not success:\n",
    "                raise IOError(f\"cv2.imwrite() failed for {png_path}\")\n",
    "\n",
    "            logging.info(f\"✅ Conversion succeeded: {dicom_path} -> {png_path}\")\n",
    "            return {\"dicom\": str(dicom_path), \"status\": \"success\"}\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ Attempt {attempt} failed for {dicom_path}: {e}\")\n",
    "            sleep(1)\n",
    "\n",
    "    logging.error(f\"❌ FAILED after {MAX_RETRIES} retries: {dicom_path}\")\n",
    "    return {\"dicom\": str(dicom_path), \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "# Convert train & val\n",
    "for split in ['train', 'val']:\n",
    "    dicom_subdir = Path(dicom_dir)\n",
    "    png_subdir = Path(png_dir) / split\n",
    "    for dcm_file in dicom_subdir.glob(\"*.dicom\"):\n",
    "        out_path = png_subdir / f\"{dcm_file.stem}.png\"\n",
    "        if not out_path.exists():\n",
    "            convert_dicom_to_png_remote(str(dcm_file), str(out_path))  # ✅ no .remote()\n",
    "\n",
    "# Convert test\n",
    "for dcm_file in Path(dicom_test_dir).glob(\"*.dicom\"):\n",
    "    out_path = Path(test_dir) / f\"{dcm_file.stem}.png\"\n",
    "    if not out_path.exists():\n",
    "        convert_dicom_to_png_remote(str(dcm_file), str(out_path))  # ✅ no .remote()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load your CSV annotation\n",
    "csv_path = root / \"train_df.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Drop rows with no findings\n",
    "# df = df[df['class_name'].notna() & (df['class_name'] != 'No finding')].copy()\n",
    "df[\"image_path\"] = df[\"image_id\"].apply(lambda x: f\"YOLO/images/train/{x}.png\")  # adjust path/format\n",
    "# df['width'] = df['x_max'] - df['x_min']\n",
    "# df['height'] = df['y_max'] - df['y_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_train = png_dir/\"train\"\n",
    "actual_pngs = {p.stem for p in (png_train).glob(\"*.png\")}\n",
    "\n",
    "# Compare with image IDs in dataframe (after No finding drop)\n",
    "df_ids = set(df[\"image_id\"])\n",
    "missing_png_ids = df_ids - actual_pngs\n",
    "\n",
    "if missing_png_ids:\n",
    "    with open(\"log_dir/missing_from_disk_after_drop.txt\", \"w\") as f:\n",
    "        for mid in sorted(missing_png_ids):\n",
    "            f.write(mid + \"\\n\")\n",
    "    logging.warning(f\"{len(missing_png_ids)} image_ids missing as PNGs on disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Map classes\n",
    "df['class_id'] = df['class_name'].astype('category').cat.codes\n",
    "class_map = dict(enumerate(df['class_name'].astype('category').cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Add fold column using GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "df['fold'] = -1\n",
    "df = df.reset_index(drop=True)\n",
    "for fold, (_, val_idx) in enumerate(gkf.split(df, groups=df['image_id'])):\n",
    "    df.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 12: Normalize bbox + save to YOLO .txt\n",
    "def save_labels(df_subset, label_split):\n",
    "    for image_id, group in df_subset.groupby(\"image_id\"):\n",
    "        label_file = label_dir / label_split / f\"{image_id}.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for _, row in group.iterrows():\n",
    "#                 x_center = ((row.x_min / row.width) + (row.x_max / row.width)) / 2\n",
    "#                 y_center = ((row.y_min / row.height) + (row.y_max / row.height)) / 2\n",
    "#                 w = (row.x_max - row.x_min) / row.width\n",
    "#                 h = (row.y_max - row.y_min) / row.height\n",
    "                f.write(f\"{row.class_id} {row.x_mid:.6f} {row.y_mid:.6f} {row.w:.6f} {row.h:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Train/val split using folds\n",
    "val_fold = 0\n",
    "val_ids = df[df['fold'] == val_fold]['image_id'].unique()\n",
    "train_ids = df[df['fold'] != val_fold]['image_id'].unique()\n",
    "\n",
    "val_ids_set = set(val_ids)\n",
    "train_ids_set = set(train_ids)\n",
    "\n",
    "# Delete files not belonging to the correct split\n",
    "for f in ['train', 'val']:\n",
    "    target_dir = png_dir / f\n",
    "    valid_ids = train_ids_set if f == 'train' else val_ids_set\n",
    "    for file in target_dir.iterdir():\n",
    "        if file.is_file() and file.stem not in valid_ids:\n",
    "            file.unlink()\n",
    "\n",
    "\n",
    "save_labels(df[df.image_id.isin(train_ids)], \"train\")\n",
    "save_labels(df[df.image_id.isin(val_ids)], \"val\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Create YAML file\n",
    "yaml_content = f\"\"\"# Lung Disease Dataset\n",
    "path: {root}\n",
    "train: images/train\n",
    "val: images/val\n",
    "nc: {len(class_map)}\n",
    "names: {list(class_map.values())}\n",
    "\"\"\"\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Current working dir:\", os.getcwd())\n",
    "# print(\"Files:\", os.listdir(\".\"))\n",
    "\n",
    "# def train_yolo(config):\n",
    "#     command = [\n",
    "#         \"python\", \"yolov5/train.py\",\n",
    "#         \"--img\", \"1280\",\n",
    "#         \"--batch\", str(config[\"batch\"]),\n",
    "#         \"--epochs\", str(config[\"epochs\"]),\n",
    "#         \"--data\", config[\"data_yaml\"],\n",
    "#         \"--weights\", \"yolov5l.pt\",\n",
    "#         \"--cache\", \"disk\"\n",
    "#     ]\n",
    "\n",
    "#     result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "#     # Save logs for inspection\n",
    "#     log_path = \"log_dir/yolo_train.log\"\n",
    "#     with open(log_path, \"w\") as f:\n",
    "#         f.write(result.stdout)\n",
    "#         f.write(result.stderr)\n",
    "\n",
    "#     print(\"=== STDOUT ===\")\n",
    "#     print(result.stdout)\n",
    "#     print(\"=== STDERR ===\")\n",
    "#     print(result.stderr)\n",
    "\n",
    "#     # Do not raise error yet — just return log info\n",
    "#     weights_path = \"yolov5/runs/train/exp/weights/best.pt\"\n",
    "#     exists = os.path.exists(weights_path)\n",
    "\n",
    "#     return {\n",
    "#         \"status\": \"done\",\n",
    "#         \"weights_found\": exists,\n",
    "#         \"weights_path\": weights_path if exists else None,\n",
    "#         \"stdout_tail\": result.stdout[-500:],\n",
    "#         \"stderr_tail\": result.stderr[-500:],\n",
    "#         \"log_file\": log_path\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "command = [\n",
    "    \"python\", \"yolov5/train.py\",\n",
    "    \"--img\", \"1280\",\n",
    "    \"--batch\", \"8\",\n",
    "    \"--epochs\", \"2\",  # keep small for test\n",
    "    \"--data\", \"/mnt/shared_dataset/YOLO/my-yolov5.yaml\",\n",
    "    \"--weights\", \"yolov5s.pt\",\n",
    "    \"--cache\", \"disk\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
